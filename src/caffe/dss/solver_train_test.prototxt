net: "train_val.prototxt"

test_iter: 153
test_interval: 8000

# lr for fine-tuning should be lower than when starting from scratch
#debug_info: true
base_lr: 1e-8
lr_policy: "step"
#power: 0.9
gamma: 0.1
iter_size: 10
# stepsize should also be lower, as we're closer to being done
stepsize: 8000
average_loss: 20
display: 20
# 30 epoch
max_iter: 800000
momentum: 0.90
weight_decay: 0.0005
snapshot: 8000
snapshot_prefix: "snapshot/ours"
# test_initialization: false
solver_mode: GPU
